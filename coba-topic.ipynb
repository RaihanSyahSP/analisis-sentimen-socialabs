{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import nltk\n",
    "import re\n",
    "import pandas as pd\n",
    "import string\n",
    "import gensim\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from openai import AzureOpenAI\n",
    "from gensim import corpora, models\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "indonesian_stopwords = set(stopwords.words('indonesian'))\n",
    "english_stopwords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Llm:\n",
    "    def getContext(topics, keyword, best_num_topics_str):\n",
    "        # Data dengan bobot kata-kata tertinggi untuk setiap topik\n",
    "        data = [topics]\n",
    "\n",
    "        # Mengolah data untuk mengambil kata-kata terbobot tertinggi\n",
    "        topic_keywords = []\n",
    "        for topic_data in data:\n",
    "            topic_data = topic_data[0][1]  # Ambil data topik pertama (bobot tertinggi)\n",
    "            topic_data = sorted(topic_data, key=lambda x: x[1], reverse=True)  # Urutkan berdasarkan bobot terbesar ke terkecil\n",
    "            keywords = [word[0] for word in topic_data[:10]]  # Ambil 10 kata dengan bobot tertinggi\n",
    "            topic_keywords.extend(keywords)\n",
    "\n",
    "        # Create a prompt using the keywords\n",
    "        role = \"AI Linguistik\"\n",
    "        action = \"menentukan kalimat dari beberapa topik berdasarkan dari kumpulan kata-kata hasil dari proses LDA\"\n",
    "        step = (\"mempertimbangkan bobot setiap topik yang ada pada penomoran angka dalam merangkai kata-kata kunci \"\n",
    "                \"menjadi kalimat yang padu untuk sebuah topik yang diperbincangkan di Twitter. \"\n",
    "                \"Dengan menggunakan penomoran untuk setiap topik, ambil kata inti dari hasil LDA lalu susun menjadi \"\n",
    "                \"kalimat yang padu dan mudah dipahami.\")\n",
    "        context = (f\"Anda akan membahas tentang topik dengan kata-kata kunci berikut: \\n\\n\"\n",
    "                f\"{' '.join(topic_keywords)} \\n\\n\"\n",
    "                f\"dengan berbagai pandangan masyarakat terhadap topik tersebut.\")\n",
    "        example = (\"Misalnya, jika hasil analisis LDA mengidentifikasi kata-kata kunci seperti 'ukt', 'mahal', 'banget', \"\n",
    "                \"'jual', 'usia', 'kemendikbudristek', 'hoyong', 'maba', 'nilainilai', 'kemaren', Anda akan menciptakan \"\n",
    "                \"kalimat seperti: 'Dalam pandangan beberapa masyarakat, kenaikan ukt dianggap sangat mahal banget dan ini \"\n",
    "                \"mempengaruhi mahasiswa baru (maba) yang merasa terbebani, terutama mereka yang belum memiliki penghasilan \"\n",
    "                \"dan masih berada pada usia muda. Beberapa orang bahkan bercanda tentang menjual barang-barang mereka untuk \"\n",
    "                \"membayar biaya pendidikan. Selain itu, Kemendikbudristek diharapkan dapat menilai kembali kebijakan ini untuk \"\n",
    "                \"mempertimbangkan nilai-nilai kesetaraan dalam akses pendidikan. Kemarin, isu ini semakin hangat diperbincangkan \"\n",
    "                \"di media sosial.'\")\n",
    "        format_str = f\"dengan format kalimat naratif berbentuk bullet point yang menggabungkan semua kata kunci tersebut dengan jumlah sesuai jumlah topik yang diberikan yaitu : {best_num_topics_str}.\"\n",
    "\n",
    "        # Combine components into the final prompt\n",
    "        prompt = (f\"# RASCEF = Role + ( Action + Step + Context + Example ) + Format\\n\\n\"\n",
    "                f\"Role: {role}\\n\"\n",
    "                f\"Action: {action}\\n\"\n",
    "                f\"Step: {step}\\n\"\n",
    "                f\"Context: {context}\\n\"\n",
    "                f\"Example: {example}\\n\"\n",
    "                f\"Format: {format_str}\\n\\n\"\n",
    "                f\"Kata-kata kunci: {', '.join(topic_keywords)}\")\n",
    "\n",
    "        # Inisialisasi Azure OpenAI client\n",
    "        client = AzureOpenAI(\n",
    "            api_version=\"2023-05-15\",\n",
    "            azure_endpoint=\"https://chatbot-aic.openai.azure.com/\",\n",
    "            api_key=\"fd068a3036e34fe188a28392699ecc65\",\n",
    "        )\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"aicdeploymodel\",  # Ganti dengan nama deployment Anda\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.3\n",
    "        )\n",
    "\n",
    "        \n",
    "        # Extract the generated sentence from the response\n",
    "        generated_sentence = response.choices[0].message.content.replace('\\n', '<br/>').replace(' .', '.').strip()\n",
    "        # generated_sentence = response\n",
    "        # Print the generated sentence\n",
    "        return generated_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:6: DeprecationWarning: invalid escape sequence \\S\n",
      "<>:13: DeprecationWarning: invalid escape sequence \\s\n",
      "<>:6: DeprecationWarning: invalid escape sequence \\S\n",
      "<>:13: DeprecationWarning: invalid escape sequence \\s\n",
      "C:\\Users\\lenovo\\AppData\\Local\\Temp\\ipykernel_20428\\132160428.py:6: DeprecationWarning: invalid escape sequence \\S\n",
      "  tweet = re.compile('https?://\\S+|www\\.\\S+').sub(r'', tweet) # Remove hyperlinks\n",
      "C:\\Users\\lenovo\\AppData\\Local\\Temp\\ipykernel_20428\\132160428.py:13: DeprecationWarning: invalid escape sequence \\s\n",
      "  tweet = re.compile('@[^\\s]+').sub(r'', tweet) # Mentions\n"
     ]
    }
   ],
   "source": [
    "class Lda:\n",
    "    def clusster(data, keyword, num_topics=5):\n",
    "\n",
    "        def clean_tweet(tweet):\n",
    "            tweet = str(tweet).lower()\n",
    "            tweet = re.compile('https?://\\S+|www\\.\\S+').sub(r'', tweet) # Remove hyperlinks\n",
    "\n",
    "            if tweet.startswith(\"rt\"): # Remove retweets (repetitions)\n",
    "                i = tweet.find(':')\n",
    "                if i != -1:\n",
    "                    tweet = tweet[i+2:]\n",
    "            \n",
    "            tweet = re.compile('@[^\\s]+').sub(r'', tweet) # Mentions \n",
    "            tweet = re.compile(r'#([^\\s]+)').sub(r'\\1', tweet) # Remove hashtags\n",
    "            tweet = re.sub('@', 'at', tweet)\n",
    "            tweet  = ''.join([char for char in tweet if char not in string.punctuation]) #Remove punctuation characters\n",
    "            tweet = re.compile('[^A-Za-z]').sub(r' ', tweet) # Remove any other non-alphabet characters\n",
    "            tweet = ' '.join([w for w in tweet.split() if w not in indonesian_stopwords]) #Remove stop words  \n",
    "            # tweet = ' '.join([w for w in tweet.split() if w not in english_stopwords]) #Remove stop words  \n",
    "            \n",
    "            return tweet\n",
    "\n",
    "        def remove_single_letter_words(text):\n",
    "            text = re.sub(r'\\b\\w\\b', '', text)\n",
    "            \n",
    "            hapus = ['id','amp','deh','tanyakanrl','dtype','length','sih','na','si','rj','lc','ar','oe','al','sm','ri','en','ar','mc','vt','rob','ny','dc','az','va','mkmk','nya','do','ye']\n",
    "            words = text.split()\n",
    "\n",
    "            # Memfilter kata-kata yang tidak ada dalam array yang akan dihapus\n",
    "            kata_kata_tanpa_kata_yang_dihapus = [kata for kata in words if kata not in hapus]\n",
    "\n",
    "            # Menggabungkan kata-kata yang tersisa menjadi kalimat baru\n",
    "            kalimat_tanpa_kata_yang_dihapus = ' '.join(kata_kata_tanpa_kata_yang_dihapus)\n",
    "            return kalimat_tanpa_kata_yang_dihapus\n",
    "\n",
    "        data = pd.DataFrame(data)\n",
    "        # data = data.apply(fix_contractions_id)\n",
    "        data = data.apply(clean_tweet)\n",
    "        data = data.apply(remove_single_letter_words)\n",
    "\n",
    "        tokenized_documents = [doc.split() for doc in data]\n",
    "        # print(tokenized_documents)\n",
    "\n",
    "        # Membuat kamus (dictionary)\n",
    "        dictionary = corpora.Dictionary(tokenized_documents)\n",
    "\n",
    "        # Membuat corpus (representasi vektor dari dokumen)\n",
    "        corpus = [dictionary.doc2bow(doc) for doc in tokenized_documents]\n",
    "\n",
    "        # Clusstering with LDA\n",
    "        lda_model = gensim.models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=20, alpha=1, eta=1)\n",
    "\n",
    "        topics = lda_model.show_topics(num_topics=num_topics, log=False, formatted=False)\n",
    "        # return topics\n",
    "        # Print the topics\n",
    "        res = []\n",
    "        for topic_id, topic in topics:\n",
    "            res.append([word for word, _ in topic])\n",
    "            print(topic)\n",
    "\n",
    "        print(res)     \n",
    "        \n",
    "        # return topics\n",
    "        return Llm.getContext(topics, keyword, num_topics)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('sapi', 0.07466889), ('india', 0.052936353), ('import', 0.05293352), ('program', 0.042103276), ('susu', 0.04210202), ('impor', 0.031322222), ('wowo', 0.020705596), ('iga', 0.02070538), ('penyet', 0.020705316), ('hamburka', 0.020705175)]\n",
      "[('sapi', 0.025287876), ('india', 0.025240202), ('import', 0.025237376), ('program', 0.025197256), ('susu', 0.025197096), ('impor', 0.02512407), ('kerbau', 0.024963653), ('subianto', 0.024962978), ('prabowo', 0.024962831), ('nomor', 0.02496282)]\n",
      "[('sapi', 0.02530226), ('import', 0.02524727), ('india', 0.025246115), ('susu', 0.025200523), ('program', 0.025200121), ('impor', 0.025124934), ('goreng', 0.024963113), ('nomor', 0.024962967), ('subianto', 0.024962265), ('indomie', 0.024962135)]\n",
      "[('sapi', 0.025283508), ('import', 0.025243348), ('india', 0.025235731), ('susu', 0.025196336), ('program', 0.025194792), ('impor', 0.025125528), ('ngeri', 0.024963744), ('pd', 0.024963576), ('goreng', 0.024963224), ('nomor', 0.024963131)]\n",
      "[('sapi', 0.025280552), ('import', 0.02523713), ('india', 0.025236769), ('susu', 0.025195904), ('program', 0.025194898), ('impor', 0.025124747), ('object', 0.024963481), ('cerdas', 0.024963416), ('penyelenggaraan', 0.02496335), ('pd', 0.02496335)]\n",
      "[['sapi', 'india', 'import', 'program', 'susu', 'impor', 'wowo', 'iga', 'penyet', 'hamburka'], ['sapi', 'india', 'import', 'program', 'susu', 'impor', 'kerbau', 'subianto', 'prabowo', 'nomor'], ['sapi', 'import', 'india', 'susu', 'program', 'impor', 'goreng', 'nomor', 'subianto', 'indomie'], ['sapi', 'import', 'india', 'susu', 'program', 'impor', 'ngeri', 'pd', 'goreng', 'nomor'], ['sapi', 'import', 'india', 'susu', 'program', 'impor', 'object', 'cerdas', 'penyelenggaraan', 'pd']]\n"
     ]
    }
   ],
   "source": [
    "csv_file_path = './dataset/dataset-testing/impor-susu-sapi-jan-mei.csv'\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "data = df['full_text']\n",
    "\n",
    "topics = Lda.clusster(data, 'impor susu', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Dalam pandangan beberapa masyarakat, program import sapi dari India dianggap sebagai ancaman bagi peternak lokal dan industri susu dalam negeri.\n",
      "2. Terdapat perdebatan tentang kebijakan impor susu yang dianggap merugikan peternak lokal dan menguntungkan impor dari luar negeri.\n",
      "3. Beberapa orang terkesan dengan keberhasilan program impor wowo yang berhasil menarik minat masyarakat untuk mencoba hidangan baru seperti iga penyet dan hamburka.\n",
      "4. Namun, ada juga yang menentang program impor wowo karena dianggap mengancam keberlangsungan hidangan tradisional Indonesia.\n",
      "5. Dalam pandangan beberapa masyarakat, kebijakan impor susu dan program impor wowo harus dipertimbangkan dengan hati-hati agar tidak merugikan industri dalam negeri dan melestarikan kekayaan kuliner Indonesia.\n"
     ]
    }
   ],
   "source": [
    "# Replace HTML line breaks with actual newlines for better readability\n",
    "topics = topics.replace('<br/>', '\\n')\n",
    "\n",
    "print(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conversation_id_str</th>\n",
       "      <th>created_at</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>full_text</th>\n",
       "      <th>id_str</th>\n",
       "      <th>lang</th>\n",
       "      <th>probability</th>\n",
       "      <th>quote_count</th>\n",
       "      <th>reply_count</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>topic</th>\n",
       "      <th>tweet_url</th>\n",
       "      <th>user_id_str</th>\n",
       "      <th>username</th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1722416236083200098</td>\n",
       "      <td>Fri Nov 10 23:49:21 +0000 2023</td>\n",
       "      <td>0</td>\n",
       "      <td>@M45Broo Gpp halu dulu ðŸ¤£ noh manusia uban data...</td>\n",
       "      <td>1723125723156947294</td>\n",
       "      <td>in</td>\n",
       "      <td>0.97057265</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>https://twitter.com/nusa_talk/status/172312572...</td>\n",
       "      <td>1571698388785336320</td>\n",
       "      <td>nusa_talk</td>\n",
       "      <td>Berikut adalah kalimat-kalimat utama untuk set...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1723109580073521507</td>\n",
       "      <td>Fri Nov 10 23:39:41 +0000 2023</td>\n",
       "      <td>1</td>\n",
       "      <td>@MinmonPS7 Terus anies harus berpasangan dgn s...</td>\n",
       "      <td>1723123292335431687</td>\n",
       "      <td>in</td>\n",
       "      <td>0.93176955</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://twitter.com/tanz_77/status/17231232923...</td>\n",
       "      <td>187094886</td>\n",
       "      <td>tanz_77</td>\n",
       "      <td>Berikut adalah kalimat-kalimat utama untuk set...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1722966988979683638</td>\n",
       "      <td>Fri Nov 10 23:37:43 +0000 2023</td>\n",
       "      <td>0</td>\n",
       "      <td>@Tan_Mar3M Jni pertanyaan jebakan.....klu berp...</td>\n",
       "      <td>1723122796518334479</td>\n",
       "      <td>in</td>\n",
       "      <td>0.9614479</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>https://twitter.com/BumiOfai/status/1723122796...</td>\n",
       "      <td>1710774321076207616</td>\n",
       "      <td>BumiOfai</td>\n",
       "      <td>Berikut adalah kalimat-kalimat utama untuk set...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1722889979721097379</td>\n",
       "      <td>Fri Nov 10 23:34:36 +0000 2023</td>\n",
       "      <td>0</td>\n",
       "      <td>@tempodotco Pak Anies Baswedan mengajarkan kit...</td>\n",
       "      <td>1723122011566923957</td>\n",
       "      <td>in</td>\n",
       "      <td>0.94221425</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>https://twitter.com/koponng/status/17231220115...</td>\n",
       "      <td>883660630816727040</td>\n",
       "      <td>koponng</td>\n",
       "      <td>Berikut adalah kalimat-kalimat utama untuk set...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1722889979721097379</td>\n",
       "      <td>Fri Nov 10 23:31:02 +0000 2023</td>\n",
       "      <td>0</td>\n",
       "      <td>@tempodotco @mengenyam AMIN mungkin meraih has...</td>\n",
       "      <td>1723121115512664185</td>\n",
       "      <td>in</td>\n",
       "      <td>0.9638936</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>https://twitter.com/sebeelum/status/1723121115...</td>\n",
       "      <td>886925187907596292</td>\n",
       "      <td>sebeelum</td>\n",
       "      <td>Berikut adalah kalimat-kalimat utama untuk set...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>1722939439385837814</td>\n",
       "      <td>Fri Nov 10 11:29:07 +0000 2023</td>\n",
       "      <td>0</td>\n",
       "      <td>Mantan Menag Fachrul Razi Sebut Anies-Muhaimin...</td>\n",
       "      <td>1722939439385837814</td>\n",
       "      <td>in</td>\n",
       "      <td>0.72561145</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>https://twitter.com/jpnncom/status/17229394393...</td>\n",
       "      <td>79130206</td>\n",
       "      <td>jpnncom</td>\n",
       "      <td>Berikut adalah kalimat-kalimat utama untuk set...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>1722939188331614355</td>\n",
       "      <td>Fri Nov 10 11:28:07 +0000 2023</td>\n",
       "      <td>0</td>\n",
       "      <td>Kemenangan Pak Anies akan menginspirasi genera...</td>\n",
       "      <td>1722939188331614355</td>\n",
       "      <td>in</td>\n",
       "      <td>0.540799</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>https://twitter.com/SriLamtiur/status/17229391...</td>\n",
       "      <td>1678989817106034690</td>\n",
       "      <td>SriLamtiur</td>\n",
       "      <td>Berikut adalah kalimat-kalimat utama untuk set...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>1722939111982653520</td>\n",
       "      <td>Fri Nov 10 11:27:49 +0000 2023</td>\n",
       "      <td>0</td>\n",
       "      <td>Pemimpin yang berkomitmen untuk kemajuan bangs...</td>\n",
       "      <td>1722939111982653520</td>\n",
       "      <td>in</td>\n",
       "      <td>0.9423321</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>https://twitter.com/wsri786031/status/17229391...</td>\n",
       "      <td>1678033921181896704</td>\n",
       "      <td>wsri786031</td>\n",
       "      <td>Berikut adalah kalimat-kalimat utama untuk set...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>1722842978543366491</td>\n",
       "      <td>Fri Nov 10 11:27:09 +0000 2023</td>\n",
       "      <td>15</td>\n",
       "      <td>@andre_rosiade Semoga menjadi amal kebaikan da...</td>\n",
       "      <td>1722938945691095497</td>\n",
       "      <td>in</td>\n",
       "      <td>0.93272656</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>https://twitter.com/Saerudin16/status/17229389...</td>\n",
       "      <td>1316456812099432449</td>\n",
       "      <td>Saerudin16</td>\n",
       "      <td>Berikut adalah kalimat-kalimat utama untuk set...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>1722938319552802842</td>\n",
       "      <td>Fri Nov 10 11:24:40 +0000 2023</td>\n",
       "      <td>0</td>\n",
       "      <td>Pak Anies Baswedan adalah pemimpin yang berpot...</td>\n",
       "      <td>1722938319552802842</td>\n",
       "      <td>in</td>\n",
       "      <td>0.93784297</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>https://twitter.com/isvaradhana/status/1722938...</td>\n",
       "      <td>1676622561881522176</td>\n",
       "      <td>isvaradhana</td>\n",
       "      <td>Berikut adalah kalimat-kalimat utama untuk set...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     conversation_id_str                      created_at  favorite_count  \\\n",
       "0    1722416236083200098  Fri Nov 10 23:49:21 +0000 2023               0   \n",
       "1    1723109580073521507  Fri Nov 10 23:39:41 +0000 2023               1   \n",
       "2    1722966988979683638  Fri Nov 10 23:37:43 +0000 2023               0   \n",
       "3    1722889979721097379  Fri Nov 10 23:34:36 +0000 2023               0   \n",
       "4    1722889979721097379  Fri Nov 10 23:31:02 +0000 2023               0   \n",
       "..                   ...                             ...             ...   \n",
       "995  1722939439385837814  Fri Nov 10 11:29:07 +0000 2023               0   \n",
       "996  1722939188331614355  Fri Nov 10 11:28:07 +0000 2023               0   \n",
       "997  1722939111982653520  Fri Nov 10 11:27:49 +0000 2023               0   \n",
       "998  1722842978543366491  Fri Nov 10 11:27:09 +0000 2023              15   \n",
       "999  1722938319552802842  Fri Nov 10 11:24:40 +0000 2023               0   \n",
       "\n",
       "                                             full_text               id_str  \\\n",
       "0    @M45Broo Gpp halu dulu ðŸ¤£ noh manusia uban data...  1723125723156947294   \n",
       "1    @MinmonPS7 Terus anies harus berpasangan dgn s...  1723123292335431687   \n",
       "2    @Tan_Mar3M Jni pertanyaan jebakan.....klu berp...  1723122796518334479   \n",
       "3    @tempodotco Pak Anies Baswedan mengajarkan kit...  1723122011566923957   \n",
       "4    @tempodotco @mengenyam AMIN mungkin meraih has...  1723121115512664185   \n",
       "..                                                 ...                  ...   \n",
       "995  Mantan Menag Fachrul Razi Sebut Anies-Muhaimin...  1722939439385837814   \n",
       "996  Kemenangan Pak Anies akan menginspirasi genera...  1722939188331614355   \n",
       "997  Pemimpin yang berkomitmen untuk kemajuan bangs...  1722939111982653520   \n",
       "998  @andre_rosiade Semoga menjadi amal kebaikan da...  1722938945691095497   \n",
       "999  Pak Anies Baswedan adalah pemimpin yang berpot...  1722938319552802842   \n",
       "\n",
       "    lang probability  quote_count  reply_count  retweet_count topic  \\\n",
       "0     in  0.97057265            0            0              0     3   \n",
       "1     in  0.93176955            0            2              0     0   \n",
       "2     in   0.9614479            0            0              0     2   \n",
       "3     in  0.94221425            0            0              0     2   \n",
       "4     in   0.9638936            0            0              0     1   \n",
       "..   ...         ...          ...          ...            ...   ...   \n",
       "995   in  0.72561145            1            0              0     4   \n",
       "996   in    0.540799            0            0              0     2   \n",
       "997   in   0.9423321            0            0              0     2   \n",
       "998   in  0.93272656            0            0              0     2   \n",
       "999   in  0.93784297            0            0              0     2   \n",
       "\n",
       "                                             tweet_url          user_id_str  \\\n",
       "0    https://twitter.com/nusa_talk/status/172312572...  1571698388785336320   \n",
       "1    https://twitter.com/tanz_77/status/17231232923...            187094886   \n",
       "2    https://twitter.com/BumiOfai/status/1723122796...  1710774321076207616   \n",
       "3    https://twitter.com/koponng/status/17231220115...   883660630816727040   \n",
       "4    https://twitter.com/sebeelum/status/1723121115...   886925187907596292   \n",
       "..                                                 ...                  ...   \n",
       "995  https://twitter.com/jpnncom/status/17229394393...             79130206   \n",
       "996  https://twitter.com/SriLamtiur/status/17229391...  1678989817106034690   \n",
       "997  https://twitter.com/wsri786031/status/17229391...  1678033921181896704   \n",
       "998  https://twitter.com/Saerudin16/status/17229389...  1316456812099432449   \n",
       "999  https://twitter.com/isvaradhana/status/1722938...  1676622561881522176   \n",
       "\n",
       "        username                                            context  \n",
       "0      nusa_talk  Berikut adalah kalimat-kalimat utama untuk set...  \n",
       "1        tanz_77  Berikut adalah kalimat-kalimat utama untuk set...  \n",
       "2       BumiOfai  Berikut adalah kalimat-kalimat utama untuk set...  \n",
       "3        koponng  Berikut adalah kalimat-kalimat utama untuk set...  \n",
       "4       sebeelum  Berikut adalah kalimat-kalimat utama untuk set...  \n",
       "..           ...                                                ...  \n",
       "995      jpnncom  Berikut adalah kalimat-kalimat utama untuk set...  \n",
       "996   SriLamtiur  Berikut adalah kalimat-kalimat utama untuk set...  \n",
       "997   wsri786031  Berikut adalah kalimat-kalimat utama untuk set...  \n",
       "998   Saerudin16  Berikut adalah kalimat-kalimat utama untuk set...  \n",
       "999  isvaradhana  Berikut adalah kalimat-kalimat utama untuk set...  \n",
       "\n",
       "[1000 rows x 15 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# Fetching data from the provided URL\n",
    "url = \"http://topic-socialabs.unikomcodelabs.id/topic?keyword=anies&num_topics=5&num_tweets=500\"\n",
    "response = requests.get(url)\n",
    "data = response.json()\n",
    "\n",
    "# Extract context\n",
    "context = data['data']['context']\n",
    "\n",
    "# Extract documents_topic\n",
    "documents_topic = data['data']['documents_topic']\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(documents_topic)\n",
    "\n",
    "# Add context to each row\n",
    "df['context'] = context\n",
    "# Saving the DataFrame to a CSV file\n",
    "csv_file_path = './dataset/dataset-testing/topik-anies.csv'\n",
    "df.to_csv(csv_file_path, index=False)\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
