{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import nltk\n",
    "import re\n",
    "import pandas as pd\n",
    "import string\n",
    "import gensim\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from openai import AzureOpenAI\n",
    "from gensim import corpora, models\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "indonesian_stopwords = set(stopwords.words('indonesian'))\n",
    "english_stopwords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Llm:\n",
    "    def getContext(topics, keyword, best_num_topics_str):\n",
    "        # Data dengan bobot kata-kata tertinggi untuk setiap topik\n",
    "        data = [topics]\n",
    "\n",
    "        # Mengolah data untuk mengambil kata-kata terbobot tertinggi\n",
    "        topic_keywords = []\n",
    "        for topic_data in data:\n",
    "            topic_data = topic_data[0][1]  # Ambil data topik pertama (bobot tertinggi)\n",
    "            topic_data = sorted(topic_data, key=lambda x: x[1], reverse=True)  # Urutkan berdasarkan bobot terbesar ke terkecil\n",
    "            keywords = [word[0] for word in topic_data[:10]]  # Ambil 10 kata dengan bobot tertinggi\n",
    "            topic_keywords.extend(keywords)\n",
    "\n",
    "        # Create a prompt using the keywords\n",
    "        role = \"AI Linguistik\"\n",
    "        action = \"menentukan kalimat dari beberapa topik berdasarkan dari kumpulan kata-kata hasil dari proses LDA\"\n",
    "        step = (\"mempertimbangkan bobot setiap topik yang ada pada penomoran angka dalam merangkai kata-kata kunci \"\n",
    "                \"menjadi kalimat yang padu untuk sebuah topik yang diperbincangkan di Twitter. \"\n",
    "                \"Dengan menggunakan penomoran untuk setiap topik, ambil kata inti dari hasil LDA lalu susun menjadi \"\n",
    "                \"kalimat yang padu dan mudah dipahami.\")\n",
    "        context = (f\"Anda akan membahas tentang topik dengan kata-kata kunci berikut: \\n\\n\"\n",
    "                f\"{' '.join(topic_keywords)} \\n\\n\"\n",
    "                f\"dengan berbagai pandangan masyarakat terhadap topik tersebut.\")\n",
    "        example = (\"Misalnya, jika hasil analisis LDA mengidentifikasi kata-kata kunci seperti 'ukt', 'mahal', 'banget', \"\n",
    "                \"'jual', 'usia', 'kemendikbudristek', 'hoyong', 'maba', 'nilainilai', 'kemaren', Anda akan menciptakan \"\n",
    "                \"kalimat seperti: 'Dalam pandangan beberapa masyarakat, kenaikan ukt dianggap sangat mahal banget dan ini \"\n",
    "                \"mempengaruhi mahasiswa baru (maba) yang merasa terbebani, terutama mereka yang belum memiliki penghasilan \"\n",
    "                \"dan masih berada pada usia muda. Beberapa orang bahkan bercanda tentang menjual barang-barang mereka untuk \"\n",
    "                \"membayar biaya pendidikan. Selain itu, Kemendikbudristek diharapkan dapat menilai kembali kebijakan ini untuk \"\n",
    "                \"mempertimbangkan nilai-nilai kesetaraan dalam akses pendidikan. Kemarin, isu ini semakin hangat diperbincangkan \"\n",
    "                \"di media sosial.'\")\n",
    "        format_str = f\"dengan format kalimat naratif berbentuk bullet point yang menggabungkan semua kata kunci tersebut dengan jumlah sesuai jumlah topik yang diberikan yaitu : {best_num_topics_str}.\"\n",
    "\n",
    "        # Combine components into the final prompt\n",
    "        prompt = (f\"# RASCEF = Role + ( Action + Step + Context + Example ) + Format\\n\\n\"\n",
    "                f\"Role: {role}\\n\"\n",
    "                f\"Action: {action}\\n\"\n",
    "                f\"Step: {step}\\n\"\n",
    "                f\"Context: {context}\\n\"\n",
    "                f\"Example: {example}\\n\"\n",
    "                f\"Format: {format_str}\\n\\n\"\n",
    "                f\"Kata-kata kunci: {', '.join(topic_keywords)}\")\n",
    "\n",
    "        # Inisialisasi Azure OpenAI client\n",
    "        client = AzureOpenAI(\n",
    "            api_version=\"2023-05-15\",\n",
    "            azure_endpoint=\"https://chatbot-aic.openai.azure.com/\",\n",
    "            api_key=\"fd068a3036e34fe188a28392699ecc65\",\n",
    "        )\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"aicdeploymodel\",  # Ganti dengan nama deployment Anda\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.3\n",
    "        )\n",
    "\n",
    "        \n",
    "        # Extract the generated sentence from the response\n",
    "        generated_sentence = response.choices[0].message.content.replace('\\n', '<br/>').replace(' .', '.').strip()\n",
    "        # generated_sentence = response\n",
    "        # Print the generated sentence\n",
    "        return generated_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:6: DeprecationWarning: invalid escape sequence \\S\n",
      "<>:13: DeprecationWarning: invalid escape sequence \\s\n",
      "<>:6: DeprecationWarning: invalid escape sequence \\S\n",
      "<>:13: DeprecationWarning: invalid escape sequence \\s\n",
      "C:\\Users\\lenovo\\AppData\\Local\\Temp\\ipykernel_20428\\132160428.py:6: DeprecationWarning: invalid escape sequence \\S\n",
      "  tweet = re.compile('https?://\\S+|www\\.\\S+').sub(r'', tweet) # Remove hyperlinks\n",
      "C:\\Users\\lenovo\\AppData\\Local\\Temp\\ipykernel_20428\\132160428.py:13: DeprecationWarning: invalid escape sequence \\s\n",
      "  tweet = re.compile('@[^\\s]+').sub(r'', tweet) # Mentions\n"
     ]
    }
   ],
   "source": [
    "class Lda:\n",
    "    def clusster(data, keyword, num_topics=5):\n",
    "\n",
    "        def clean_tweet(tweet):\n",
    "            tweet = str(tweet).lower()\n",
    "            tweet = re.compile('https?://\\S+|www\\.\\S+').sub(r'', tweet) # Remove hyperlinks\n",
    "\n",
    "            if tweet.startswith(\"rt\"): # Remove retweets (repetitions)\n",
    "                i = tweet.find(':')\n",
    "                if i != -1:\n",
    "                    tweet = tweet[i+2:]\n",
    "            \n",
    "            tweet = re.compile('@[^\\s]+').sub(r'', tweet) # Mentions \n",
    "            tweet = re.compile(r'#([^\\s]+)').sub(r'\\1', tweet) # Remove hashtags\n",
    "            tweet = re.sub('@', 'at', tweet)\n",
    "            tweet  = ''.join([char for char in tweet if char not in string.punctuation]) #Remove punctuation characters\n",
    "            tweet = re.compile('[^A-Za-z]').sub(r' ', tweet) # Remove any other non-alphabet characters\n",
    "            tweet = ' '.join([w for w in tweet.split() if w not in indonesian_stopwords]) #Remove stop words  \n",
    "            # tweet = ' '.join([w for w in tweet.split() if w not in english_stopwords]) #Remove stop words  \n",
    "            \n",
    "            return tweet\n",
    "\n",
    "        def remove_single_letter_words(text):\n",
    "            text = re.sub(r'\\b\\w\\b', '', text)\n",
    "            \n",
    "            hapus = ['id','amp','deh','tanyakanrl','dtype','length','sih','na','si','rj','lc','ar','oe','al','sm','ri','en','ar','mc','vt','rob','ny','dc','az','va','mkmk','nya','do','ye']\n",
    "            words = text.split()\n",
    "\n",
    "            # Memfilter kata-kata yang tidak ada dalam array yang akan dihapus\n",
    "            kata_kata_tanpa_kata_yang_dihapus = [kata for kata in words if kata not in hapus]\n",
    "\n",
    "            # Menggabungkan kata-kata yang tersisa menjadi kalimat baru\n",
    "            kalimat_tanpa_kata_yang_dihapus = ' '.join(kata_kata_tanpa_kata_yang_dihapus)\n",
    "            return kalimat_tanpa_kata_yang_dihapus\n",
    "\n",
    "        data = pd.DataFrame(data)\n",
    "        # data = data.apply(fix_contractions_id)\n",
    "        data = data.apply(clean_tweet)\n",
    "        data = data.apply(remove_single_letter_words)\n",
    "\n",
    "        tokenized_documents = [doc.split() for doc in data]\n",
    "        # print(tokenized_documents)\n",
    "\n",
    "        # Membuat kamus (dictionary)\n",
    "        dictionary = corpora.Dictionary(tokenized_documents)\n",
    "\n",
    "        # Membuat corpus (representasi vektor dari dokumen)\n",
    "        corpus = [dictionary.doc2bow(doc) for doc in tokenized_documents]\n",
    "\n",
    "        # Clusstering with LDA\n",
    "        lda_model = gensim.models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=20, alpha=1, eta=1)\n",
    "\n",
    "        topics = lda_model.show_topics(num_topics=num_topics, log=False, formatted=False)\n",
    "        # return topics\n",
    "        # Print the topics\n",
    "        res = []\n",
    "        for topic_id, topic in topics:\n",
    "            res.append([word for word, _ in topic])\n",
    "            print(topic)\n",
    "\n",
    "        print(res)     \n",
    "        \n",
    "        # return topics\n",
    "        return Llm.getContext(topics, keyword, num_topics)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('sapi', 0.07466889), ('india', 0.052936353), ('import', 0.05293352), ('program', 0.042103276), ('susu', 0.04210202), ('impor', 0.031322222), ('wowo', 0.020705596), ('iga', 0.02070538), ('penyet', 0.020705316), ('hamburka', 0.020705175)]\n",
      "[('sapi', 0.025287876), ('india', 0.025240202), ('import', 0.025237376), ('program', 0.025197256), ('susu', 0.025197096), ('impor', 0.02512407), ('kerbau', 0.024963653), ('subianto', 0.024962978), ('prabowo', 0.024962831), ('nomor', 0.02496282)]\n",
      "[('sapi', 0.02530226), ('import', 0.02524727), ('india', 0.025246115), ('susu', 0.025200523), ('program', 0.025200121), ('impor', 0.025124934), ('goreng', 0.024963113), ('nomor', 0.024962967), ('subianto', 0.024962265), ('indomie', 0.024962135)]\n",
      "[('sapi', 0.025283508), ('import', 0.025243348), ('india', 0.025235731), ('susu', 0.025196336), ('program', 0.025194792), ('impor', 0.025125528), ('ngeri', 0.024963744), ('pd', 0.024963576), ('goreng', 0.024963224), ('nomor', 0.024963131)]\n",
      "[('sapi', 0.025280552), ('import', 0.02523713), ('india', 0.025236769), ('susu', 0.025195904), ('program', 0.025194898), ('impor', 0.025124747), ('object', 0.024963481), ('cerdas', 0.024963416), ('penyelenggaraan', 0.02496335), ('pd', 0.02496335)]\n",
      "[['sapi', 'india', 'import', 'program', 'susu', 'impor', 'wowo', 'iga', 'penyet', 'hamburka'], ['sapi', 'india', 'import', 'program', 'susu', 'impor', 'kerbau', 'subianto', 'prabowo', 'nomor'], ['sapi', 'import', 'india', 'susu', 'program', 'impor', 'goreng', 'nomor', 'subianto', 'indomie'], ['sapi', 'import', 'india', 'susu', 'program', 'impor', 'ngeri', 'pd', 'goreng', 'nomor'], ['sapi', 'import', 'india', 'susu', 'program', 'impor', 'object', 'cerdas', 'penyelenggaraan', 'pd']]\n"
     ]
    }
   ],
   "source": [
    "csv_file_path = './dataset/dataset-testing/impor-susu-sapi-jan-mei.csv'\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "data = df['full_text']\n",
    "\n",
    "topics = Lda.clusster(data, 'impor susu', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Dalam pandangan beberapa masyarakat, program import sapi dari India dianggap sebagai ancaman bagi peternak lokal dan industri susu dalam negeri.\n",
      "2. Terdapat perdebatan tentang kebijakan impor susu yang dianggap merugikan peternak lokal dan menguntungkan impor dari luar negeri.\n",
      "3. Beberapa orang terkesan dengan keberhasilan program impor wowo yang berhasil menarik minat masyarakat untuk mencoba hidangan baru seperti iga penyet dan hamburka.\n",
      "4. Namun, ada juga yang menentang program impor wowo karena dianggap mengancam keberlangsungan hidangan tradisional Indonesia.\n",
      "5. Dalam pandangan beberapa masyarakat, kebijakan impor susu dan program impor wowo harus dipertimbangkan dengan hati-hati agar tidak merugikan industri dalam negeri dan melestarikan kekayaan kuliner Indonesia.\n"
     ]
    }
   ],
   "source": [
    "# Replace HTML line breaks with actual newlines for better readability\n",
    "topics = topics.replace('<br/>', '\\n')\n",
    "\n",
    "print(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
