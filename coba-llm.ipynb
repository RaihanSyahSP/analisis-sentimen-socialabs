{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from openai import AzureOpenAI\n",
    "import tiktoken\n",
    "\n",
    "class Llm:\n",
    "    @staticmethod\n",
    "    def summarize_sentiments(csv_file_path_positif, csv_file_path_negatif):\n",
    "        # Membaca data dari CSV\n",
    "        df_positif = pd.read_csv(csv_file_path_positif)\n",
    "        df_negatif = pd.read_csv(csv_file_path_negatif)\n",
    "          # Menghapus baris dengan nilai NaN di kolom 'processed'\n",
    "        df_positif = df_positif.dropna(subset=['processed'])\n",
    "        df_negatif = df_negatif.dropna(subset=['processed'])\n",
    "        \n",
    "        # Memisahkan teks berdasarkan sentimen\n",
    "        positive_texts = df_positif['processed'].tolist()\n",
    "        negative_texts = df_negatif['processed'].tolist()\n",
    "\n",
    "        # TF-IDF vectorization\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        positive_tfidf = vectorizer.fit_transform(positive_texts)\n",
    "        negative_tfidf = vectorizer.fit_transform(negative_texts)\n",
    "        print(positive_tfidf.shape)\n",
    "        \n",
    "        # Mengambil fitur (kata-kata) dan skor tf-idf tertinggi\n",
    "        def extract_top_keywords(tfidf_matrix, feature_names, top_n=100):\n",
    "            top_keywords = {}\n",
    "            for row in tfidf_matrix:\n",
    "                indices = row.indices\n",
    "                scores = row.data\n",
    "                for index, score in zip(indices, scores):\n",
    "                    if feature_names[index] in top_keywords:\n",
    "                        top_keywords[feature_names[index]] = max(top_keywords[feature_names[index]], score)\n",
    "                    else:\n",
    "                        top_keywords[feature_names[index]] = score\n",
    "            \n",
    "            sorted_keywords = sorted(top_keywords.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
    "            return \" \".join([word for word, score in sorted_keywords])\n",
    "        \n",
    "        positive_top_keywords = extract_top_keywords(positive_tfidf, vectorizer.get_feature_names_out())\n",
    "        negative_top_keywords = extract_top_keywords(negative_tfidf, vectorizer.get_feature_names_out())\n",
    "\n",
    "        print(positive_top_keywords)\n",
    "        print(len(positive_top_keywords))\n",
    "        print(negative_top_keywords)\n",
    "        print(len(negative_top_keywords))\n",
    "\n",
    "        # Define max tokens per request and tokenizer\n",
    "        max_tokens = 2000  # Adjust to a reasonable size to balance chunking and API call overhead\n",
    "        tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "        # Function to split text into chunks based on token count\n",
    "        def split_text_into_chunks(text, max_tokens, tokenizer):\n",
    "            words = text.split()\n",
    "            chunks = []\n",
    "            current_chunk = []\n",
    "\n",
    "            for word in words:\n",
    "                current_chunk.append(word)\n",
    "                if len(tokenizer.encode(\" \".join(current_chunk))) > max_tokens:\n",
    "                    current_chunk.pop()\n",
    "                    chunks.append(\" \".join(current_chunk))\n",
    "                    current_chunk = [word]\n",
    "\n",
    "            if current_chunk:\n",
    "                chunks.append(\" \".join(current_chunk))\n",
    "\n",
    "            return chunks\n",
    "\n",
    "        # Create prompts and process each chunk\n",
    "        def create_prompt(sentiment, summary_text, keyword):\n",
    "            role = \"AI Linguistik\"\n",
    "            action = \"Tambahkan konteks tambahan yang menjelaskan mengapa pandangan tertentu dipegang oleh sebagian orang berdasarkan sentimen positif atau negatif terhadap suatu topik\"\n",
    "            step = \"menganalisis teks hasil preprocessing untuk menemukan kata-kata kunci yang paling relevan dan menyusun kalimat yang memberikan konteks tambahan\"\n",
    "            context = f\"menambahkan konteks tambahan mengapa orang-orang memandang {sentiment} terhadap topik {keyword}\"\n",
    "            example = f\"Misalnya, jika teks memiliki sentimen {sentiment}, Anda akan menambahkan konteks tambahan dengan mengambil 3 poin utama mengapa orang-orang memandang {sentiment} terhadap topik ini. Jelaskan alasan-alasan ini dengan menghubungkannya ke kata-kata kunci yang paling relevan.\"\n",
    "            format_str = \"dalam format poin-poin bullet\"\n",
    "\n",
    "            # Combine components into the RASCEF prompt\n",
    "            prompt = (f\"# RASCEF = Role + ( Action + Step + Context + Example ) + Format\\n\\n\"\n",
    "                    f\"Anda adalah {role} yang {action} {step} {context}. {example} Buatkan {format_str} dari teks berikut:\\n\\n\"\n",
    "                    f\"{summary_text}\")\n",
    "            return prompt\n",
    "\n",
    "        def summarize_chunks(sentiment, chunks, keyword):\n",
    "            summaries = []\n",
    "            for chunk in chunks:\n",
    "                prompt = create_prompt(sentiment, chunk, keyword)\n",
    "                response = client.chat.completions.create(\n",
    "                    model=\"aicdeploymodel\",  # Replace with your deployment name\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                        {\"role\": \"user\", \"content\": prompt}\n",
    "                    ],\n",
    "                    max_tokens=100,\n",
    "                    temperature=0.3\n",
    "                )\n",
    "                summary = response.choices[0].message.content.strip()\n",
    "                summaries.append(summary)\n",
    "            return \" \".join(summaries)\n",
    "\n",
    "        # Initialize Azure OpenAI client\n",
    "        client = AzureOpenAI(\n",
    "            api_version=\"2023-05-15\",\n",
    "            azure_endpoint=\"https://chatbot-aic.openai.azure.com/\",\n",
    "            api_key=\"fd068a3036e34fe188a28392699ecc65\",\n",
    "        )\n",
    "\n",
    "        # Generate summary for positive sentiment\n",
    "        positive_chunks = split_text_into_chunks(positive_top_keywords, max_tokens, tokenizer)\n",
    "        positive_summary = summarize_chunks('positif', positive_chunks, \"impor susu\")\n",
    "\n",
    "        # Generate summary for negative sentiment\n",
    "        negative_chunks = split_text_into_chunks(negative_top_keywords, max_tokens, tokenizer)\n",
    "        negative_summary = summarize_chunks('negatif', negative_chunks, \"impor susu\")\n",
    "\n",
    "        # Return summaries for both sentiments\n",
    "        return {\n",
    "            'positive_summary': positive_summary,\n",
    "            'negative_summary': negative_summary\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99, 679)\n",
      "jelek ideal after debat alasan ailan gede komponen dagingnya korban burung kuning idul dana kuota cegah bumn aman hukum ketahanan hasil konstitusi basah jakarta cuannya dpet karantina boyolali barang kerjanya konsepnya lelang dibahas dalem bisnis durung handle let bea celeng gandum diledek kecil jelas bilang diprotes jutaan hamil ikam ajak berdampak ayo bem diam jamin laman cik gjlas beranak buruh india ambisius lactose hutang berpmk australia elite intoleran dukungan listrik jenis ahli didengarkan kelen badan kelilmuannya kuliah hasto jagung acak goblog lambat babi gegara blunder ketua bapakkau hubungan cocok cengengas food gratisblablabla kandungan kanada harga demand dibantu ditenderkan gyuldangies folat\n",
      "716\n",
      "hayo ranum disuruh perah bilang benar bukan marah su beras pangan mencre anak tahun kasihan hmm jpg mendukung juta jack shi anjing penyediaan omon kata selandia bahaya pang jangan bisnis dibully orang kecil kompor kerbau asia india import rakyat impornya permudah mengurusi contohnya menyambung dsn berlaku produknya daftar pmk ngempeng solusi hidup banjiri awas mengeluarkan diseantero capek berhasil protes lo produsen papua apa this bebas diskresi kedelai mentah batasi sampir gratisnya pajak via mengurus maggot pakan satunya pemasukan pers keterangan maret bumn komoditas ijin pemerintah impor oknum peternak prabowogibran id kanada dorong bapanas penuhi panas mempermudah critany ripil pes mehh\n",
      "700\n",
      "Positive Summary:\n",
      " Konteks tambahan mengapa orang-orang memandang positif terhadap topik impor susu:\n",
      "\n",
      "- Impor susu dapat membantu memenuhi kebutuhan susu di dalam negeri yang tidak dapat dipenuhi oleh produksi dalam negeri.\n",
      "- Impor susu dapat membantu mengurangi harga susu di dalam negeri yang cenderung lebih mahal karena biaya produksi yang tinggi.\n",
      "- Impor susu dapat membantu\n",
      "Negative Summary:\n",
      " Konteks tambahan mengapa orang-orang memandang negatif terhadap topik impor susu:\n",
      "\n",
      "- Ada kekhawatiran tentang kualitas susu impor yang tidak diawasi dengan ketat oleh pemerintah, terutama susu yang berasal dari negara-negara dengan standar keamanan pangan yang rendah seperti India dan China.\n",
      "- Impor susu juga dianggap dapat merugikan petern\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "summaries = Llm.summarize_sentiments('./hasil-klasifikasi/impor_susu_sapi_jan_mei_pos.csv', './hasil-klasifikasi/impor_susu_sapi_jan_mei_neg.csv')\n",
    "print(\"Positive Summary:\\n\", summaries['positive_summary'])\n",
    "print(\"Negative Summary:\\n\", summaries['negative_summary'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
