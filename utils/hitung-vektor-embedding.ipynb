{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<OOV>': 1, 'bagus': 2, 'timnas': 3, 'indonesia': 4, 'main': 5, 'mantap': 6, 'keren': 7, 'banget': 8, 'tahan': 9, 'serang': 10}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sample_teks = \"timnas indonesia main mantap keren banget tahan bagus serang bagus\"\n",
    "\n",
    "# Memisahkan teks menjadi kata-kata\n",
    "kata_kata = sample_teks.split()\n",
    "\n",
    "# Inisialisasi Tokenizer\n",
    "tokenizer = Tokenizer(num_words=100, oov_token='<OOV>')\n",
    "\n",
    "# Fit teks pada tokenizer\n",
    "tokenizer.fit_on_texts(kata_kata)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "print(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timnas indonesia main mantap keren banget tahan bagus serang bagus\n",
      "[3, 4, 5, 6, 7, 8, 9, 2, 10, 2]\n"
     ]
    }
   ],
   "source": [
    "text_sequences = tokenizer.texts_to_sequences([sample_teks])[0]\n",
    "print(sample_teks)\n",
    "print(text_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ubah list menjadi array numpy\n",
    "text_sequences_array = np.array(text_sequences)\n",
    "text_sequences_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(len(word_index) + 1, 8, input_shape=(10,), embeddings_initializer='uniform'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 10)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Menambahkan dimensi tambahan ke array\n",
    "# text_sequences_array_expanded = np.expand_dims(text_sequences_array, axis=0)\n",
    "# text_sequences_array_expanded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n",
      "[[[-0.04550374 -0.00837178 -0.01312106 -0.0063603  -0.01672603\n",
      "    0.0203802   0.02742959  0.02175787]\n",
      "  [-0.03447998 -0.01891717 -0.0289578   0.00302847 -0.03514106\n",
      "    0.04604175  0.01941648 -0.04966583]\n",
      "  [ 0.00555872  0.04359151  0.00444311  0.02589743  0.04958758\n",
      "    0.00798961 -0.039342    0.0145882 ]\n",
      "  [-0.02273577 -0.03395368 -0.01961151  0.03118222 -0.02350597\n",
      "   -0.01414061 -0.00999259  0.02961962]\n",
      "  [ 0.02457335 -0.04313856 -0.01648206  0.03935889  0.02587168\n",
      "    0.03600855 -0.02544801  0.03927585]\n",
      "  [-0.02560535 -0.01044355  0.04961004 -0.04908165  0.04102651\n",
      "    0.02758962  0.03322483 -0.01463032]\n",
      "  [ 0.04609929  0.04453552  0.00779549 -0.02719733 -0.01365424\n",
      "   -0.04052832  0.04487603 -0.03613282]\n",
      "  [ 0.03928185 -0.02329862 -0.04917803  0.04597708 -0.00145281\n",
      "    0.01015916 -0.0147874  -0.0033261 ]\n",
      "  [-0.03182508  0.03929876 -0.0297678  -0.0059512  -0.02487149\n",
      "   -0.01315031 -0.0395348  -0.03583873]\n",
      "  [ 0.03928185 -0.02329862 -0.04917803  0.04597708 -0.00145281\n",
      "    0.01015916 -0.0147874  -0.0033261 ]]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# output_array = model.predict(text_sequences_array_expanded)\n",
    "# print(output_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<OOV>': 1, 'bagus': 2, 'timnas': 3, 'indonesia': 4, 'mainnya': 5, 'mantap': 6, 'keren': 7, 'banget': 8, 'tahan': 9, 'serang': 10}\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "\n",
    "sample_teks = \"timnas indonesia mainnya mantap keren banget tahan bagus serang bagus\"\n",
    "\n",
    "# Memisahkan teks menjadi kata-kata\n",
    "kata_kata = [sample_teks.split()]\n",
    "\n",
    "# Inisialisasi Tokenizer\n",
    "tokenizer = Tokenizer(num_words=100, oov_token='<OOV>')\n",
    "\n",
    "# Fit teks pada tokenizer\n",
    "tokenizer.fit_on_texts(kata_kata)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "print(word_index)\n",
    "\n",
    "# Mengubah teks menjadi token\n",
    "text_sequences = tokenizer.texts_to_sequences(kata_kata)\n",
    "\n",
    "# Model Word2Vec\n",
    "w2v_model = Word2Vec(sentences=[sample_teks.split()], vector_size=8, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Create a weight matrix for the embedding layer\n",
    "vocab_size = len(word_index) + 1  # tambahkan 1 karena indeks dimulai dari 1\n",
    "embedding_matrix = np.zeros((vocab_size, 8))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if word in w2v_model.wv:\n",
    "        embedding_matrix[i] = w2v_model.wv[word]\n",
    "\n",
    "# Mencari kata berdasarkan vektor tertentu\n",
    "def find_word_by_vector(embedding_matrix, vector):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if np.array_equal(embedding_matrix[index], vector):\n",
    "            return word, embedding_matrix[index]\n",
    "    return None\n",
    "\n",
    "\n",
    "# word = find_word_by_vector(embedding_matrix, embedding_matrix[8])\n",
    "# print(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('<OOV>', array([0., 0., 0., 0., 0., 0., 0., 0.]))\n",
      "('bagus', array([-0.00670284,  0.00295539,  0.06379187,  0.11261591, -0.11628687,\n",
      "       -0.08896011,  0.08073591,  0.11216235]))\n",
      "('timnas', array([-0.0188501 ,  0.03087243, -0.01110034,  0.06917077, -0.03428721,\n",
      "        0.02825081,  0.06819743,  0.10432442]))\n",
      "('indonesia', array([ 0.05646876, -0.084837  , -0.04435611,  0.11748135, -0.01972066,\n",
      "        0.00401714, -0.05175787, -0.0960336 ]))\n",
      "('mainnya', array([ 0.06227572,  0.11541429, -0.10197397,  0.05619748, -0.05171345,\n",
      "        0.0103067 ,  0.10623275, -0.05577721]))\n",
      "('mantap', array([-0.12004438,  0.06259117, -0.10949482, -0.05489781, -0.00043875,\n",
      "       -0.00370227, -0.0957655 ,  0.12018429]))\n",
      "('keren', array([ 0.11922649, -0.09148958, -0.02917211, -0.02422176,  0.10096796,\n",
      "       -0.0741362 ,  0.00056453, -0.05942167]))\n",
      "('banget', array([ 0.07938613, -0.04256707, -0.01183002,  0.07210717, -0.09402047,\n",
      "       -0.04920129, -0.09389478, -0.01162553]))\n",
      "('tahan', array([ 0.03595725,  0.01239842, -0.10356519, -0.11811022,  0.09139708,\n",
      "        0.06337827,  0.08447117,  0.00953582]))\n",
      "('serang', array([-0.06269285, -0.04704215,  0.09225631, -0.01916839, -0.05670767,\n",
      "        0.08192565, -0.060752  , -0.02270022]))\n"
     ]
    }
   ],
   "source": [
    "# Contoh: Mencari kata untuk vektor embedding_matrix[3]\n",
    "for i in range(1, 11):\n",
    "    word = find_word_by_vector(embedding_matrix, embedding_matrix[i])\n",
    "    print(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;OOV&gt;</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bagus</td>\n",
       "      <td>[-0.01, 0.0, 0.06, 0.11, -0.12, -0.09, 0.08, 0.11]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>timnas</td>\n",
       "      <td>[-0.02, 0.03, -0.01, 0.07, -0.03, 0.03, 0.07, 0.1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>indonesia</td>\n",
       "      <td>[0.06, -0.08, -0.04, 0.12, -0.02, 0.0, -0.05, -0.1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>main</td>\n",
       "      <td>[0.06, 0.12, -0.1, 0.06, -0.05, 0.01, 0.11, -0.06]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>mantap</td>\n",
       "      <td>[-0.12, 0.06, -0.11, -0.05, -0.0, -0.0, -0.1, 0.12]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>keren</td>\n",
       "      <td>[0.12, -0.09, -0.03, -0.02, 0.1, -0.07, 0.0, -0.06]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>banget</td>\n",
       "      <td>[0.08, -0.04, -0.01, 0.07, -0.09, -0.05, -0.09, -0.01]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>tahan</td>\n",
       "      <td>[0.04, 0.01, -0.1, -0.12, 0.09, 0.06, 0.08, 0.01]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>serang</td>\n",
       "      <td>[-0.06, -0.05, 0.09, -0.02, -0.06, 0.08, -0.06, -0.02]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Word                                                  Vector\n",
       "0      <OOV>                [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
       "1      bagus      [-0.01, 0.0, 0.06, 0.11, -0.12, -0.09, 0.08, 0.11]\n",
       "2     timnas      [-0.02, 0.03, -0.01, 0.07, -0.03, 0.03, 0.07, 0.1]\n",
       "3  indonesia     [0.06, -0.08, -0.04, 0.12, -0.02, 0.0, -0.05, -0.1]\n",
       "4       main      [0.06, 0.12, -0.1, 0.06, -0.05, 0.01, 0.11, -0.06]\n",
       "5     mantap     [-0.12, 0.06, -0.11, -0.05, -0.0, -0.0, -0.1, 0.12]\n",
       "6      keren     [0.12, -0.09, -0.03, -0.02, 0.1, -0.07, 0.0, -0.06]\n",
       "7     banget  [0.08, -0.04, -0.01, 0.07, -0.09, -0.05, -0.09, -0.01]\n",
       "8      tahan       [0.04, 0.01, -0.1, -0.12, 0.09, 0.06, 0.08, 0.01]\n",
       "9     serang  [-0.06, -0.05, 0.09, -0.02, -0.06, 0.08, -0.06, -0.02]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Bulatkan nilai-nilai dalam matriks embedding\n",
    "rounded_embedding_matrix = np.round(embedding_matrix, decimals=2)\n",
    "\n",
    "# Contoh: Mencari kata untuk vektor rounded_embedding_matrix[1]\n",
    "word_list = []\n",
    "vector_list = []\n",
    "for i in range(1, 11):\n",
    "    word, vector = find_word_by_vector(rounded_embedding_matrix, rounded_embedding_matrix[i])\n",
    "    word_list.append(word)\n",
    "    vector_list.append(vector)\n",
    "\n",
    "# Membuat DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Word': word_list,\n",
    "    'Vector': vector_list\n",
    "})\n",
    "\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel hasil inisialisasi Glorot uniform (setelah dibulatkan):\n",
      "[[ 0.23  0.53  0.45 -0.05  0.67  0.19 -0.17 -0.13]\n",
      " [ 0.19 -0.58 -0.03 -0.39 -0.18  0.03  0.21  0.68]\n",
      " [ 0.63 -0.45 -0.24  0.07 -0.52 -0.29 -0.23  0.36]\n",
      " [-0.26  0.58 -0.25  0.38 -0.41  0.07 -0.64  0.17]\n",
      " [-0.26  0.65  0.65  0.27 -0.19  0.16 -0.53  0.37]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Inisialisasi Glorot uniform untuk kernel\n",
    "initializer = tf.keras.initializers.GlorotUniform()\n",
    "\n",
    "# Membuat kernel dengan ukuran 5x8\n",
    "kernel_size = (5, 8)\n",
    "kernel = initializer(shape=kernel_size)\n",
    "\n",
    "# Memperoleh nilai kernel yang sudah dibulatkan\n",
    "rounded_kernel = np.round(kernel, decimals=2)\n",
    "\n",
    "print(\"Kernel hasil inisialisasi Glorot uniform (setelah dibulatkan):\")\n",
    "print(rounded_kernel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teks dengan negasi: Saya tidak suka makanan ini . Rasanya tidak enak .\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.sentiment.util import mark_negation\n",
    "\n",
    "# Contoh teks\n",
    "teks = \"Saya tidak suka makanan ini. Rasanya tidak enak.\"\n",
    "\n",
    "# Tokenisasi teks\n",
    "kata_kata = word_tokenize(teks)\n",
    "\n",
    "# Tandai negasi dalam teks\n",
    "kata_kata_dengan_negasi = mark_negation(kata_kata)\n",
    "\n",
    "# Gabungkan kembali kata-kata menjadi teks\n",
    "teks_dengan_negasi = ' '.join(kata_kata_dengan_negasi)\n",
    "\n",
    "print(\"Teks dengan negasi:\", teks_dengan_negasi)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
