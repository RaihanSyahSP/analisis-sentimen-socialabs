{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import nltk\n",
    "import re\n",
    "import pandas as pd\n",
    "import string\n",
    "import gensim\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from openai import AzureOpenAI\n",
    "from gensim import corpora, models\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "indonesian_stopwords = set(stopwords.words('indonesian'))\n",
    "english_stopwords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Llm:\n",
    "    def getContext(topics, keyword, best_num_topics_str):\n",
    "        # Data dengan bobot kata-kata tertinggi untuk setiap topik\n",
    "        data = [topics]\n",
    "\n",
    "        # Mengolah data untuk mengambil kata-kata terbobot tertinggi\n",
    "        topic_keywords = []\n",
    "        for topic_data in data:\n",
    "            topic_data = topic_data[0][1]  # Ambil data topik pertama (bobot tertinggi)\n",
    "            topic_data = sorted(topic_data, key=lambda x: x[1], reverse=True)  # Urutkan berdasarkan bobot terbesar ke terkecil\n",
    "            keywords = [word[0] for word in topic_data[:10]]  # Ambil 10 kata dengan bobot tertinggi\n",
    "            topic_keywords.extend(keywords)\n",
    "\n",
    "        # Create a prompt using the keywords\n",
    "        role = \"AI Linguistik\"\n",
    "        action = \"menentukan kalimat dari beberapa topik berdasarkan dari kumpulan kata-kata hasil dari proses LDA\"\n",
    "        step = (\"mempertimbangkan bobot setiap topik yang ada pada penomoran angka dalam merangkai kata-kata kunci \"\n",
    "                \"menjadi kalimat yang padu untuk sebuah topik yang diperbincangkan di Twitter. \"\n",
    "                \"Dengan menggunakan penomoran untuk setiap topik, ambil kata inti dari hasil LDA lalu susun menjadi \"\n",
    "                \"kalimat yang padu dan mudah dipahami.\")\n",
    "        context = (f\"Anda akan membahas tentang topik dengan kata-kata kunci berikut: \\n\\n\"\n",
    "                f\"{' '.join(topic_keywords)} \\n\\n\"\n",
    "                f\"dengan berbagai pandangan masyarakat terhadap topik tersebut.\")\n",
    "        example = (\"Misalnya, jika hasil analisis LDA mengidentifikasi kata-kata kunci seperti 'ukt', 'mahal', 'banget', \"\n",
    "                \"'jual', 'usia', 'kemendikbudristek', 'hoyong', 'maba', 'nilainilai', 'kemaren', Anda akan menciptakan \"\n",
    "                \"kalimat seperti: 'Dalam pandangan beberapa masyarakat, kenaikan ukt dianggap sangat mahal banget dan ini \"\n",
    "                \"mempengaruhi mahasiswa baru (maba) yang merasa terbebani, terutama mereka yang belum memiliki penghasilan \"\n",
    "                \"dan masih berada pada usia muda. Beberapa orang bahkan bercanda tentang menjual barang-barang mereka untuk \"\n",
    "                \"membayar biaya pendidikan. Selain itu, Kemendikbudristek diharapkan dapat menilai kembali kebijakan ini untuk \"\n",
    "                \"mempertimbangkan nilai-nilai kesetaraan dalam akses pendidikan. Kemarin, isu ini semakin hangat diperbincangkan \"\n",
    "                \"di media sosial.'\")\n",
    "        format_str = f\"dengan format kalimat naratif berbentuk bullet point yang menggabungkan semua kata kunci tersebut dengan jumlah sesuai jumlah topik yang diberikan yaitu : {best_num_topics_str}.\"\n",
    "\n",
    "        # Combine components into the final prompt\n",
    "        prompt = (f\"# RASCEF = Role + ( Action + Step + Context + Example ) + Format\\n\\n\"\n",
    "                f\"Role: {role}\\n\"\n",
    "                f\"Action: {action}\\n\"\n",
    "                f\"Step: {step}\\n\"\n",
    "                f\"Context: {context}\\n\"\n",
    "                f\"Example: {example}\\n\"\n",
    "                f\"Format: {format_str}\\n\\n\"\n",
    "                f\"Kata-kata kunci: {', '.join(topic_keywords)}\")\n",
    "\n",
    "        # Inisialisasi Azure OpenAI client\n",
    "        client = AzureOpenAI(\n",
    "            api_version=\"2023-05-15\",\n",
    "            azure_endpoint=\"https://chatbot-aic.openai.azure.com/\",\n",
    "            api_key=\"fd068a3036e34fe188a28392699ecc65\",\n",
    "        )\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"aicdeploymodel\",  # Ganti dengan nama deployment Anda\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.3\n",
    "        )\n",
    "\n",
    "        \n",
    "        # Extract the generated sentence from the response\n",
    "        generated_sentence = response.choices[0].message.content.replace('\\n', '<br/>').replace(' .', '.').strip()\n",
    "        # generated_sentence = response\n",
    "        # Print the generated sentence\n",
    "        return generated_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:6: DeprecationWarning: invalid escape sequence \\S\n",
      "<>:13: DeprecationWarning: invalid escape sequence \\s\n",
      "<>:6: DeprecationWarning: invalid escape sequence \\S\n",
      "<>:13: DeprecationWarning: invalid escape sequence \\s\n",
      "C:\\Users\\lenovo\\AppData\\Local\\Temp\\ipykernel_20428\\132160428.py:6: DeprecationWarning: invalid escape sequence \\S\n",
      "  tweet = re.compile('https?://\\S+|www\\.\\S+').sub(r'', tweet) # Remove hyperlinks\n",
      "C:\\Users\\lenovo\\AppData\\Local\\Temp\\ipykernel_20428\\132160428.py:13: DeprecationWarning: invalid escape sequence \\s\n",
      "  tweet = re.compile('@[^\\s]+').sub(r'', tweet) # Mentions\n"
     ]
    }
   ],
   "source": [
    "class Lda:\n",
    "    def clusster(data, keyword, num_topics=5):\n",
    "\n",
    "        def clean_tweet(tweet):\n",
    "            tweet = str(tweet).lower()\n",
    "            tweet = re.compile('https?://\\S+|www\\.\\S+').sub(r'', tweet) # Remove hyperlinks\n",
    "\n",
    "            if tweet.startswith(\"rt\"): # Remove retweets (repetitions)\n",
    "                i = tweet.find(':')\n",
    "                if i != -1:\n",
    "                    tweet = tweet[i+2:]\n",
    "            \n",
    "            tweet = re.compile('@[^\\s]+').sub(r'', tweet) # Mentions \n",
    "            tweet = re.compile(r'#([^\\s]+)').sub(r'\\1', tweet) # Remove hashtags\n",
    "            tweet = re.sub('@', 'at', tweet)\n",
    "            tweet  = ''.join([char for char in tweet if char not in string.punctuation]) #Remove punctuation characters\n",
    "            tweet = re.compile('[^A-Za-z]').sub(r' ', tweet) # Remove any other non-alphabet characters\n",
    "            tweet = ' '.join([w for w in tweet.split() if w not in indonesian_stopwords]) #Remove stop words  \n",
    "            # tweet = ' '.join([w for w in tweet.split() if w not in english_stopwords]) #Remove stop words  \n",
    "            \n",
    "            return tweet\n",
    "\n",
    "        def remove_single_letter_words(text):\n",
    "            text = re.sub(r'\\b\\w\\b', '', text)\n",
    "            \n",
    "            hapus = ['id','amp','deh','tanyakanrl','dtype','length','sih','na','si','rj','lc','ar','oe','al','sm','ri','en','ar','mc','vt','rob','ny','dc','az','va','mkmk','nya','do','ye']\n",
    "            words = text.split()\n",
    "\n",
    "            # Memfilter kata-kata yang tidak ada dalam array yang akan dihapus\n",
    "            kata_kata_tanpa_kata_yang_dihapus = [kata for kata in words if kata not in hapus]\n",
    "\n",
    "            # Menggabungkan kata-kata yang tersisa menjadi kalimat baru\n",
    "            kalimat_tanpa_kata_yang_dihapus = ' '.join(kata_kata_tanpa_kata_yang_dihapus)\n",
    "            return kalimat_tanpa_kata_yang_dihapus\n",
    "\n",
    "        data = pd.DataFrame(data)\n",
    "        # data = data.apply(fix_contractions_id)\n",
    "        data = data.apply(clean_tweet)\n",
    "        data = data.apply(remove_single_letter_words)\n",
    "\n",
    "        tokenized_documents = [doc.split() for doc in data]\n",
    "        # print(tokenized_documents)\n",
    "\n",
    "        # Membuat kamus (dictionary)\n",
    "        dictionary = corpora.Dictionary(tokenized_documents)\n",
    "\n",
    "        # Membuat corpus (representasi vektor dari dokumen)\n",
    "        corpus = [dictionary.doc2bow(doc) for doc in tokenized_documents]\n",
    "\n",
    "        # Clusstering with LDA\n",
    "        lda_model = gensim.models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=20, alpha=1, eta=1)\n",
    "\n",
    "        topics = lda_model.show_topics(num_topics=num_topics, log=False, formatted=False)\n",
    "        # return topics\n",
    "        # Print the topics\n",
    "        res = []\n",
    "        for topic_id, topic in topics:\n",
    "            res.append([word for word, _ in topic])\n",
    "            print(topic)\n",
    "\n",
    "        print(res)     \n",
    "        \n",
    "        # return topics\n",
    "        return Llm.getContext(topics, keyword, num_topics)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('sapi', 0.07466889), ('india', 0.052936353), ('import', 0.05293352), ('program', 0.042103276), ('susu', 0.04210202), ('impor', 0.031322222), ('wowo', 0.020705596), ('iga', 0.02070538), ('penyet', 0.020705316), ('hamburka', 0.020705175)]\n",
      "[('sapi', 0.025287876), ('india', 0.025240202), ('import', 0.025237376), ('program', 0.025197256), ('susu', 0.025197096), ('impor', 0.02512407), ('kerbau', 0.024963653), ('subianto', 0.024962978), ('prabowo', 0.024962831), ('nomor', 0.02496282)]\n",
      "[('sapi', 0.02530226), ('import', 0.02524727), ('india', 0.025246115), ('susu', 0.025200523), ('program', 0.025200121), ('impor', 0.025124934), ('goreng', 0.024963113), ('nomor', 0.024962967), ('subianto', 0.024962265), ('indomie', 0.024962135)]\n",
      "[('sapi', 0.025283508), ('import', 0.025243348), ('india', 0.025235731), ('susu', 0.025196336), ('program', 0.025194792), ('impor', 0.025125528), ('ngeri', 0.024963744), ('pd', 0.024963576), ('goreng', 0.024963224), ('nomor', 0.024963131)]\n",
      "[('sapi', 0.025280552), ('import', 0.02523713), ('india', 0.025236769), ('susu', 0.025195904), ('program', 0.025194898), ('impor', 0.025124747), ('object', 0.024963481), ('cerdas', 0.024963416), ('penyelenggaraan', 0.02496335), ('pd', 0.02496335)]\n",
      "[['sapi', 'india', 'import', 'program', 'susu', 'impor', 'wowo', 'iga', 'penyet', 'hamburka'], ['sapi', 'india', 'import', 'program', 'susu', 'impor', 'kerbau', 'subianto', 'prabowo', 'nomor'], ['sapi', 'import', 'india', 'susu', 'program', 'impor', 'goreng', 'nomor', 'subianto', 'indomie'], ['sapi', 'import', 'india', 'susu', 'program', 'impor', 'ngeri', 'pd', 'goreng', 'nomor'], ['sapi', 'import', 'india', 'susu', 'program', 'impor', 'object', 'cerdas', 'penyelenggaraan', 'pd']]\n"
     ]
    }
   ],
   "source": [
    "csv_file_path = './dataset/dataset-testing/impor-susu-sapi-jan-mei.csv'\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "data = df['full_text']\n",
    "\n",
    "topics = Lda.clusster(data, 'impor susu', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Dalam pandangan beberapa masyarakat, program import sapi dari India dianggap sebagai ancaman bagi peternak lokal dan industri susu dalam negeri.\n",
      "2. Terdapat perdebatan tentang kebijakan impor susu yang dianggap merugikan peternak lokal dan menguntungkan impor dari luar negeri.\n",
      "3. Beberapa orang terkesan dengan keberhasilan program impor wowo yang berhasil menarik minat masyarakat untuk mencoba hidangan baru seperti iga penyet dan hamburka.\n",
      "4. Namun, ada juga yang menentang program impor wowo karena dianggap mengancam keberlangsungan hidangan tradisional Indonesia.\n",
      "5. Dalam pandangan beberapa masyarakat, kebijakan impor susu dan program impor wowo harus dipertimbangkan dengan hati-hati agar tidak merugikan industri dalam negeri dan melestarikan kekayaan kuliner Indonesia.\n"
     ]
    }
   ],
   "source": [
    "# Replace HTML line breaks with actual newlines for better readability\n",
    "topics = topics.replace('<br/>', '\\n')\n",
    "\n",
    "print(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conversation_id_str</th>\n",
       "      <th>created_at</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>full_text</th>\n",
       "      <th>id_str</th>\n",
       "      <th>image_url</th>\n",
       "      <th>in_reply_to_screen_name</th>\n",
       "      <th>lang</th>\n",
       "      <th>location</th>\n",
       "      <th>probability</th>\n",
       "      <th>quote_count</th>\n",
       "      <th>reply_count</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>topic</th>\n",
       "      <th>tweet_url</th>\n",
       "      <th>user_id_str</th>\n",
       "      <th>username</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.800000e+18</td>\n",
       "      <td>Tue May 28 21:59:28 +0000 2024</td>\n",
       "      <td>0</td>\n",
       "      <td>@nanawourld yg iniii pls bagus bgt! klo ga sal...</td>\n",
       "      <td>1.263182e+18</td>\n",
       "      <td>https://pbs.twimg.com/media/GOsqhc4bcAYL2hv.jpg</td>\n",
       "      <td>nanawourld</td>\n",
       "      <td>in</td>\n",
       "      <td>֪   ֪   ֪</td>\n",
       "      <td>0.5242981</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>https://x.com/qfloryn/status/1795575645927534898</td>\n",
       "      <td>2.315959e+09</td>\n",
       "      <td>qfloryn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.800000e+18</td>\n",
       "      <td>Mon May 27 23:40:08 +0000 2024</td>\n",
       "      <td>0</td>\n",
       "      <td>@ohmybeautybank kalo tekstur sih iyaa bisa tp ...</td>\n",
       "      <td>8.697740e+18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ohmybeautybank</td>\n",
       "      <td>in</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.95456576</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>https://x.com/dayooee/status/1795238588822728895</td>\n",
       "      <td>8.520000e+17</td>\n",
       "      <td>dayooee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.790000e+18</td>\n",
       "      <td>Mon May 27 00:48:44 +0000 2024</td>\n",
       "      <td>0</td>\n",
       "      <td>Kayanya moist cosrx kemarin terlalu heavy deh ...</td>\n",
       "      <td>1.778676e+18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>chrispypaw</td>\n",
       "      <td>in</td>\n",
       "      <td>stayville</td>\n",
       "      <td>0.9260342</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>https://x.com/chrispypaw/status/17948934651108...</td>\n",
       "      <td>1.190000e+18</td>\n",
       "      <td>chrispypaw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.790000e+18</td>\n",
       "      <td>Sun May 26 11:46:58 +0000 2024</td>\n",
       "      <td>0</td>\n",
       "      <td>@tanyarlfes moist cosrx 200k. alias lumayan bg...</td>\n",
       "      <td>9.725252e+18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>tanyarlfes</td>\n",
       "      <td>in</td>\n",
       "      <td>notknown</td>\n",
       "      <td>0.9367903</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://x.com/jalapinokio/status/1794696727351...</td>\n",
       "      <td>1.220000e+18</td>\n",
       "      <td>jalapinokio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.790000e+18</td>\n",
       "      <td>Sat May 25 06:08:13 +0000 2024</td>\n",
       "      <td>1</td>\n",
       "      <td>@beauthingy St aku normal to oily pake moist c...</td>\n",
       "      <td>6.613074e+18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>beauthingy</td>\n",
       "      <td>in</td>\n",
       "      <td>mars</td>\n",
       "      <td>0.96151936</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>https://x.com/urstrawbewwy/status/179424909186...</td>\n",
       "      <td>1.450000e+18</td>\n",
       "      <td>urstrawbewwy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452</th>\n",
       "      <td>1.720000e+18</td>\n",
       "      <td>Mon Nov 06 10:39:04 +0000 2023</td>\n",
       "      <td>5</td>\n",
       "      <td>moist COSRX Oil free moisturizing Moist yg ba...</td>\n",
       "      <td>6.490677e+18</td>\n",
       "      <td>https://pbs.twimg.com/media/F-PqeIraEAAyEc4.jpg</td>\n",
       "      <td>loiiyachan</td>\n",
       "      <td>in</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7641768</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>https://x.com/loiiyachan/status/17214772935610...</td>\n",
       "      <td>1.680000e+18</td>\n",
       "      <td>loiiyachan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>453</th>\n",
       "      <td>1.710000e+18</td>\n",
       "      <td>Thu Oct 12 13:37:48 +0000 2023</td>\n",
       "      <td>0</td>\n",
       "      <td>@ohmybeautybank Lagi suka sama moist cosrx wal...</td>\n",
       "      <td>9.376571e+18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ohmybeautybank</td>\n",
       "      <td>in</td>\n",
       "      <td>magic shop</td>\n",
       "      <td>0.948565</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://x.com/purpllespace/status/171246257544...</td>\n",
       "      <td>1.290000e+18</td>\n",
       "      <td>purpllespace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>454</th>\n",
       "      <td>1.710000e+18</td>\n",
       "      <td>Wed Oct 11 01:22:29 +0000 2023</td>\n",
       "      <td>0</td>\n",
       "      <td>@fidia_lintangg aku kemarinnya pake hadalabo m...</td>\n",
       "      <td>1.899338e+18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fidia_lintangg</td>\n",
       "      <td>in</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.51626235</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>https://x.com/raintikhujan/status/171191514001...</td>\n",
       "      <td>1.400000e+18</td>\n",
       "      <td>raintikhujan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>455</th>\n",
       "      <td>1.710000e+18</td>\n",
       "      <td>Thu Oct 05 16:55:19 +0000 2023</td>\n",
       "      <td>0</td>\n",
       "      <td>sejauh ini moist terbaik tuh (buat aku) natrep...</td>\n",
       "      <td>9.862582e+18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>in</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.9790384</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://x.com/isfjgurll/status/170997556776293...</td>\n",
       "      <td>1.353870e+09</td>\n",
       "      <td>isfjgurll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456</th>\n",
       "      <td>1.720000e+18</td>\n",
       "      <td>Tue Nov 07 22:40:46 +0000 2023</td>\n",
       "      <td>1</td>\n",
       "      <td>@ohmybeautybank ini progress kulit aku setelah...</td>\n",
       "      <td>1.543862e+18</td>\n",
       "      <td>https://pbs.twimg.com/media/F-XZPuqbYAARENX.jpg</td>\n",
       "      <td>ohmybeautybank</td>\n",
       "      <td>in</td>\n",
       "      <td>Kota Surabaya, Jawa Timur</td>\n",
       "      <td>0.6476847</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>https://x.com/sktiaa/status/1722021300007551011</td>\n",
       "      <td>3.230761e+09</td>\n",
       "      <td>sktiaa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>457 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     conversation_id_str                      created_at  favorite_count  \\\n",
       "0           1.800000e+18  Tue May 28 21:59:28 +0000 2024               0   \n",
       "1           1.800000e+18  Mon May 27 23:40:08 +0000 2024               0   \n",
       "2           1.790000e+18  Mon May 27 00:48:44 +0000 2024               0   \n",
       "3           1.790000e+18  Sun May 26 11:46:58 +0000 2024               0   \n",
       "4           1.790000e+18  Sat May 25 06:08:13 +0000 2024               1   \n",
       "..                   ...                             ...             ...   \n",
       "452         1.720000e+18  Mon Nov 06 10:39:04 +0000 2023               5   \n",
       "453         1.710000e+18  Thu Oct 12 13:37:48 +0000 2023               0   \n",
       "454         1.710000e+18  Wed Oct 11 01:22:29 +0000 2023               0   \n",
       "455         1.710000e+18  Thu Oct 05 16:55:19 +0000 2023               0   \n",
       "456         1.720000e+18  Tue Nov 07 22:40:46 +0000 2023               1   \n",
       "\n",
       "                                             full_text        id_str  \\\n",
       "0    @nanawourld yg iniii pls bagus bgt! klo ga sal...  1.263182e+18   \n",
       "1    @ohmybeautybank kalo tekstur sih iyaa bisa tp ...  8.697740e+18   \n",
       "2    Kayanya moist cosrx kemarin terlalu heavy deh ...  1.778676e+18   \n",
       "3    @tanyarlfes moist cosrx 200k. alias lumayan bg...  9.725252e+18   \n",
       "4    @beauthingy St aku normal to oily pake moist c...  6.613074e+18   \n",
       "..                                                 ...           ...   \n",
       "452   moist COSRX Oil free moisturizing Moist yg ba...  6.490677e+18   \n",
       "453  @ohmybeautybank Lagi suka sama moist cosrx wal...  9.376571e+18   \n",
       "454  @fidia_lintangg aku kemarinnya pake hadalabo m...  1.899338e+18   \n",
       "455  sejauh ini moist terbaik tuh (buat aku) natrep...  9.862582e+18   \n",
       "456  @ohmybeautybank ini progress kulit aku setelah...  1.543862e+18   \n",
       "\n",
       "                                           image_url in_reply_to_screen_name  \\\n",
       "0    https://pbs.twimg.com/media/GOsqhc4bcAYL2hv.jpg              nanawourld   \n",
       "1                                                NaN          ohmybeautybank   \n",
       "2                                                NaN              chrispypaw   \n",
       "3                                                NaN              tanyarlfes   \n",
       "4                                                NaN              beauthingy   \n",
       "..                                               ...                     ...   \n",
       "452  https://pbs.twimg.com/media/F-PqeIraEAAyEc4.jpg              loiiyachan   \n",
       "453                                              NaN          ohmybeautybank   \n",
       "454                                              NaN          fidia_lintangg   \n",
       "455                                              NaN                     NaN   \n",
       "456  https://pbs.twimg.com/media/F-XZPuqbYAARENX.jpg          ohmybeautybank   \n",
       "\n",
       "    lang                   location probability  quote_count  reply_count  \\\n",
       "0     in                 ֪   ֪   ֪    0.5242981            0            1   \n",
       "1     in                        NaN  0.95456576            0            0   \n",
       "2     in                  stayville   0.9260342            0            0   \n",
       "3     in                   notknown   0.9367903            0            0   \n",
       "4     in                       mars  0.96151936            0            0   \n",
       "..   ...                        ...         ...          ...          ...   \n",
       "452   in                        NaN   0.7641768            1            1   \n",
       "453   in                 magic shop    0.948565            0            0   \n",
       "454   in                        NaN  0.51626235            0            0   \n",
       "455   in                        NaN   0.9790384            0            2   \n",
       "456   in  Kota Surabaya, Jawa Timur   0.6476847            0            0   \n",
       "\n",
       "     retweet_count topic                                          tweet_url  \\\n",
       "0                0     4   https://x.com/qfloryn/status/1795575645927534898   \n",
       "1                0     4   https://x.com/dayooee/status/1795238588822728895   \n",
       "2                0     2  https://x.com/chrispypaw/status/17948934651108...   \n",
       "3                0     0  https://x.com/jalapinokio/status/1794696727351...   \n",
       "4                0     3  https://x.com/urstrawbewwy/status/179424909186...   \n",
       "..             ...   ...                                                ...   \n",
       "452              1     4  https://x.com/loiiyachan/status/17214772935610...   \n",
       "453              0     0  https://x.com/purpllespace/status/171246257544...   \n",
       "454              0     2  https://x.com/raintikhujan/status/171191514001...   \n",
       "455              0     0  https://x.com/isfjgurll/status/170997556776293...   \n",
       "456              0     4    https://x.com/sktiaa/status/1722021300007551011   \n",
       "\n",
       "      user_id_str      username  \n",
       "0    2.315959e+09       qfloryn  \n",
       "1    8.520000e+17       dayooee  \n",
       "2    1.190000e+18    chrispypaw  \n",
       "3    1.220000e+18   jalapinokio  \n",
       "4    1.450000e+18  urstrawbewwy  \n",
       "..            ...           ...  \n",
       "452  1.680000e+18    loiiyachan  \n",
       "453  1.290000e+18  purpllespace  \n",
       "454  1.400000e+18  raintikhujan  \n",
       "455  1.353870e+09     isfjgurll  \n",
       "456  3.230761e+09        sktiaa  \n",
       "\n",
       "[457 rows x 17 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# Fetching data from the provided URL\n",
    "url = \"http://topic-socialabs.unikomcodelabs.id/topic?keyword=moist%20cosrx&num_topics=5&num_tweets=500\"\n",
    "response = requests.get(url)\n",
    "data = response.json()\n",
    "\n",
    "# Extract context\n",
    "context = data['data']['context']\n",
    "\n",
    "# Extract documents_topic\n",
    "documents_topic = data['data']['documents_topic']\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(documents_topic)\n",
    "\n",
    "# # Add context to each row\n",
    "# df['context'] = context\n",
    "# # Saving the DataFrame to a CSV file\n",
    "# csv_file_path = './moist-cosrx-topik-cluster.csv'\n",
    "# df.to_csv(csv_file_path, index=False)\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "topic\n",
       "2    142\n",
       "4    130\n",
       "3    129\n",
       "1     39\n",
       "0     17\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hitung tweet berdasarkan topik nya\n",
    "df['topic'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
